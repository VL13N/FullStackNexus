ğ—µğ—®ğ˜€ğ—² 1 â€“ Subâ€Pillar Definitions
â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
1. Create a new file at `/services/pillars.js`.
2. Export four objects: `technicalMetrics`, `socialMetrics`, `fundamentalMetrics`, `astrologyMetrics`.
   â€¢ `technicalMetrics` subdivided into:
     â€“ `shortTermTrends`: ['ema8', 'ema21', 'sma50', 'sma200']
     â€“ `momentumOscillators`: ['rsi_1h', 'rsi_4h', 'macd_1h', 'macd_4h']
     â€“ `volatilityMeasures`: ['bollingerWidth_1h', 'atr_1h', 'vwapSpread']
     â€“ `orderBookLiquidity`: ['bookDepthImbalance', 'dexCexVolumeRatio']
   â€¢ `socialMetrics` subdivided into:
     â€“ `engagement`: ['socialVolume', 'tweetCount', 'telegramPostVolume']
     â€“ `sentiment`: ['lunarcrushSentiment', 'twitterPolarity']
     â€“ `influence`: ['galaxyScore', 'whaleTxCount']
     â€“ `newsFlow`: ['cryptoNewsHeadlineCount', 'githubReleaseNewsCount']
   â€¢ `fundamentalMetrics` subdivided into:
     â€“ `marketSupply`: ['marketCapUsd', 'circulatingSupplyPct', 'fullyDilutedValuation']
     â€“ `onChainUsage`: ['tps', 'activeAddresses', 'stakingYield', 'defiTvl', 'whaleFlowUsd']
     â€“ `devActivity`: ['githubCommitsCount', 'githubPullsCount']
     â€“ `macroFlows`: ['btcDominance', 'totalCryptoMarketCapExStablecoins']
   â€¢ `astrologyMetrics` subdivided into:
     â€“ `lunar`: ['lunarPhasePercentile', 'lunarPerigeeApogeeDist']
     â€“ `aspects`: ['saturnJupiterAspect', 'marsSunAspect', 'nodeSolanaLongitude']
     â€“ `ingress`: ['solarIngressAries', 'solarIngressLibra', 'nodeIngressData']
     â€“ `fixedStar`: ['siriusRisingIndicator', 'aldebaranConjunctionIndicator']
3. Above each object, add a short comment explaining why these metrics belong in that subâ€pillar.
4. Once `/services/pillars.js` is saved and consistent with planned `/api` return names, move to Phase 2.

â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
ğ—£ğ—µğ—®ğ˜€ğ—² 2 â€“ Historical Data & Normalization
â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
1. Create a script at `/scripts/fetchHistorical.js` that:
   a. Imports the metric lists from `/services/pillars.js`.
   b. For each metric, fetches its raw values hourly for the past 365 days:
      â€“ TAAPI for EMA, RSI, MACD, Bollinger, ATR, VWAP, etc.
      â€“ LunarCrush for `socialVolume`, `galaxyScore`, `sentiment`, etc.
      â€“ CryptoRank for `marketCapUsd`, `circulatingSupplyPct`, historical price.
      â€“ Onâ€chain APIs (Solana Tracker or Bitquery) for `tps`, `activeAddresses`, `defiTvl`, etc.
      â€“ Swiss Ephemeris (`sweph` package) to compute `lunarPhasePercentile` and `lunarPerigeeApogeeDist`.
   c. Writes each record into a Supabase table called `historical_metrics` with columns: 
      `(timestamp TIMESTAMP, metric_name TEXT, raw_value FLOAT)`.
   d. Throttle calls (e.g., sleep 200 ms between requests) to avoid hitting rate limits.
2. Create `/services/normalize.js`:
   a. Write a function `fitNormalization(historicalArray: number[]): { min: number, max: number }` that computes min/max from a list of raw values.
   b. Write `normalizeToScore(value: number, min: number, max: number): number` â†’ maps raw to 0â€“100 (clamp outside boundaries).
   c. Write `async initializeNormalization()` that:
      â€“ Queries Supabase for each distinct `metric_name` in `historical_metrics`.
      â€“ For each, retrieves the full array of raw_values (ordered) and runs `fitNormalization`.
      â€“ Saves results into an in-memory `normBounds` object:  
        `{ rsi_1h: { min: X, max: Y }, macd_1h: { min: A, max: B }, â€¦ }`.
   d. Write `normalizeMetrics(rawMetrics: Record<string, number>): Record<string, number>` that:
      â€“ For each key in `rawMetrics`, looks up `normBounds[key]` and calls `normalizeToScore`.
      â€“ Returns the normalized dictionary.
3. After writing `/services/normalize.js`, call `initializeNormalization()` once (e.g., in a quick standalone run) to populate `normBounds`. Ensure `normBounds` matches expected ranges (spotâ€check a few).

â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
ğ—£ğ—µğ—®ğ˜€ğ—² 3 â€“ Feature Selection & Subâ€Pillar Scoring
â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
1. Run a correlation analysis outside the agent (or in a Python notebook) to identify which normalized subâ€features actually correlate with â€œnextâ€hour % change.â€  For each hour in your historical range:
   a. Query `historical_metrics` and pivot so you have a table of shape:
      `timestamp | rsi_1h | macd_1h | ema8 | â€¦ | lunarPhasePercentile | â€¦ | price_usd`.
   b. Compute a new column `nextHourPct = (price_usd[t + 1] â€“ price_usd[t]) / price_usd[t] * 100`.
   c. Normalize all subâ€features via `normalizeToScore` (using your fitted bounds).
   d. Compute Pearson correlation (or mutual info) between each normalized subâ€feature and `nextHourPct`.
   e. Pick the top 2â€“4 subâ€features per subâ€pillar. Discard the rest.
2. Create `/services/scorers.js` with functions to compute each subâ€pillarâ€™s composite subâ€score:
   a. For example, if â€œmomentumOscillatorsâ€ kept only `rsi_1h` and `macd_1h`, write:
   ```js
   export function computeMomentumScore(norm) {
     // Example weights discovered via correlation: RSI 70%, MACD 30%
     return 0.7 * norm.rsi_1h + 0.3 * norm.macd_1h;
   }

b. Repeat for each subâ€pillar group you retained (trendScore, volatilityScore, liquidityScore, engagementScore, sentimentScore, influenceScore, newsScore, marketSupplyScore, onChainUsageScore, devActivityScore, macroFlowsScore, lunarScore, aspectsScore, ingressScore, fixedStarScore).
c. Then export composite functions:

export function computeTechnicalScore(norm) {
  const m = computeMomentumScore(norm);
  const t = computeTrendScore(norm);
  const v = computeVolatilityScore(norm);
  const l = computeLiquidityScore(norm);
  // Example intraâ€technical weights (from Phase 3 analysis):
  return 0.3 * m + 0.25 * t + 0.25 * v + 0.2 * l;
}
// Similarly, computeSocialScore(), computeFundamentalScore(), computeAstrologyScore()