𝗵𝗮𝘀𝗲 1 – Sub‐Pillar Definitions
––––––––––––––––––––––––––––––––––––––––––––––––––––––––
1. Create a new file at `/services/pillars.js`.
2. Export four objects: `technicalMetrics`, `socialMetrics`, `fundamentalMetrics`, `astrologyMetrics`.
   • `technicalMetrics` subdivided into:
     – `shortTermTrends`: ['ema8', 'ema21', 'sma50', 'sma200']
     – `momentumOscillators`: ['rsi_1h', 'rsi_4h', 'macd_1h', 'macd_4h']
     – `volatilityMeasures`: ['bollingerWidth_1h', 'atr_1h', 'vwapSpread']
     – `orderBookLiquidity`: ['bookDepthImbalance', 'dexCexVolumeRatio']
   • `socialMetrics` subdivided into:
     – `engagement`: ['socialVolume', 'tweetCount', 'telegramPostVolume']
     – `sentiment`: ['lunarcrushSentiment', 'twitterPolarity']
     – `influence`: ['galaxyScore', 'whaleTxCount']
     – `newsFlow`: ['cryptoNewsHeadlineCount', 'githubReleaseNewsCount']
   • `fundamentalMetrics` subdivided into:
     – `marketSupply`: ['marketCapUsd', 'circulatingSupplyPct', 'fullyDilutedValuation']
     – `onChainUsage`: ['tps', 'activeAddresses', 'stakingYield', 'defiTvl', 'whaleFlowUsd']
     – `devActivity`: ['githubCommitsCount', 'githubPullsCount']
     – `macroFlows`: ['btcDominance', 'totalCryptoMarketCapExStablecoins']
   • `astrologyMetrics` subdivided into:
     – `lunar`: ['lunarPhasePercentile', 'lunarPerigeeApogeeDist']
     – `aspects`: ['saturnJupiterAspect', 'marsSunAspect', 'nodeSolanaLongitude']
     – `ingress`: ['solarIngressAries', 'solarIngressLibra', 'nodeIngressData']
     – `fixedStar`: ['siriusRisingIndicator', 'aldebaranConjunctionIndicator']
3. Above each object, add a short comment explaining why these metrics belong in that sub‐pillar.
4. Once `/services/pillars.js` is saved and consistent with planned `/api` return names, move to Phase 2.

––––––––––––––––––––––––––––––––––––––––––––––––––––––––
𝗣𝗵𝗮𝘀𝗲 2 – Historical Data & Normalization
––––––––––––––––––––––––––––––––––––––––––––––––––––––––
1. Create a script at `/scripts/fetchHistorical.js` that:
   a. Imports the metric lists from `/services/pillars.js`.
   b. For each metric, fetches its raw values hourly for the past 365 days:
      – TAAPI for EMA, RSI, MACD, Bollinger, ATR, VWAP, etc.
      – LunarCrush for `socialVolume`, `galaxyScore`, `sentiment`, etc.
      – CryptoRank for `marketCapUsd`, `circulatingSupplyPct`, historical price.
      – On‐chain APIs (Solana Tracker or Bitquery) for `tps`, `activeAddresses`, `defiTvl`, etc.
      – Swiss Ephemeris (`sweph` package) to compute `lunarPhasePercentile` and `lunarPerigeeApogeeDist`.
   c. Writes each record into a Supabase table called `historical_metrics` with columns: 
      `(timestamp TIMESTAMP, metric_name TEXT, raw_value FLOAT)`.
   d. Throttle calls (e.g., sleep 200 ms between requests) to avoid hitting rate limits.
2. Create `/services/normalize.js`:
   a. Write a function `fitNormalization(historicalArray: number[]): { min: number, max: number }` that computes min/max from a list of raw values.
   b. Write `normalizeToScore(value: number, min: number, max: number): number` → maps raw to 0–100 (clamp outside boundaries).
   c. Write `async initializeNormalization()` that:
      – Queries Supabase for each distinct `metric_name` in `historical_metrics`.
      – For each, retrieves the full array of raw_values (ordered) and runs `fitNormalization`.
      – Saves results into an in-memory `normBounds` object:  
        `{ rsi_1h: { min: X, max: Y }, macd_1h: { min: A, max: B }, … }`.
   d. Write `normalizeMetrics(rawMetrics: Record<string, number>): Record<string, number>` that:
      – For each key in `rawMetrics`, looks up `normBounds[key]` and calls `normalizeToScore`.
      – Returns the normalized dictionary.
3. After writing `/services/normalize.js`, call `initializeNormalization()` once (e.g., in a quick standalone run) to populate `normBounds`. Ensure `normBounds` matches expected ranges (spot‐check a few).

––––––––––––––––––––––––––––––––––––––––––––––––––––––––
𝗣𝗵𝗮𝘀𝗲 3 – Feature Selection & Sub‐Pillar Scoring
––––––––––––––––––––––––––––––––––––––––––––––––––––––––
1. Run a correlation analysis outside the agent (or in a Python notebook) to identify which normalized sub‐features actually correlate with “next‐hour % change.”  For each hour in your historical range:
   a. Query `historical_metrics` and pivot so you have a table of shape:
      `timestamp | rsi_1h | macd_1h | ema8 | … | lunarPhasePercentile | … | price_usd`.
   b. Compute a new column `nextHourPct = (price_usd[t + 1] – price_usd[t]) / price_usd[t] * 100`.
   c. Normalize all sub‐features via `normalizeToScore` (using your fitted bounds).
   d. Compute Pearson correlation (or mutual info) between each normalized sub‐feature and `nextHourPct`.
   e. Pick the top 2–4 sub‐features per sub‐pillar. Discard the rest.
2. Create `/services/scorers.js` with functions to compute each sub‐pillar’s composite sub‐score:
   a. For example, if “momentumOscillators” kept only `rsi_1h` and `macd_1h`, write:
   ```js
   export function computeMomentumScore(norm) {
     // Example weights discovered via correlation: RSI 70%, MACD 30%
     return 0.7 * norm.rsi_1h + 0.3 * norm.macd_1h;
   }

b. Repeat for each sub‐pillar group you retained (trendScore, volatilityScore, liquidityScore, engagementScore, sentimentScore, influenceScore, newsScore, marketSupplyScore, onChainUsageScore, devActivityScore, macroFlowsScore, lunarScore, aspectsScore, ingressScore, fixedStarScore).
c. Then export composite functions:

export function computeTechnicalScore(norm) {
  const m = computeMomentumScore(norm);
  const t = computeTrendScore(norm);
  const v = computeVolatilityScore(norm);
  const l = computeLiquidityScore(norm);
  // Example intra‐technical weights (from Phase 3 analysis):
  return 0.3 * m + 0.25 * t + 0.25 * v + 0.2 * l;
}
// Similarly, computeSocialScore(), computeFundamentalScore(), computeAstrologyScore()