Below is a comprehensive plan for capturing every API response into Supabase so that you’ll have a complete, timestamped history of all inputs and outputs for future machine‐learning or backtesting. We’ll cover:

    Recommended Supabase schema (tables + columns)

    Modifying your existing services to write each API response into Supabase

    Indexing and best practices for efficient storage and future ML workloads

    How to query/pull that data later for training models

1. Supabase Schema Design

We want a table for each “pillar” (technical, social, fundamental, on-chain, astrology, financial-astrology), plus tables for predictions, news scores, daily updates, and dynamic weights. Each table should:

    Store the raw JSON (or key fields) of the API response

    Include a timestamp for when it was fetched

    Include identifying fields (e.g. coin_symbol, interval) so you know exactly what data you have

    Have a surrogate primary key (UUID or serial) and, optionally, a foreign key if needed

Below is a suggested schema. Adjust column types or naming to match your existing conventions.
1.1. technical_data (TAAPI Pro)

-- Supabase SQL (run in SQL editor)

create table public.technical_data (
  id                uuid                 primary key default uuid_generate_v4(),
  fetched_at        timestamptz          not null default now(),
  coin_symbol       text                 not null,    -- e.g. 'SOL/USDT'
  interval          text                 not null,    -- e.g. '1h', '4h'
  raw_response      jsonb                not null,    -- store full JSON from TAAPI bulk
  rsi_value         numeric(6,3),                     -- optional: parsed RSI
  macd_histogram    numeric(10,5),                     -- optional: parsed MACD.histogram
  ema_value         numeric(10,5),                     -- optional: parsed EMA value
  -- add more parsed columns as needed
  constraint chk_coin_symbol check (coin_symbol <> '')
);

create index idx_technical_data_fetched_at on public.technical_data (fetched_at);
create index idx_technical_data_symbol_interval on public.technical_data (coin_symbol, interval);

    raw_response jsonb: the entire JSON returned by TAAPI’s bulk endpoint

    Optional parsed columns (rsi_value, macd_histogram, etc.) so you can query without unnesting JSON. You may add more columns (VWAP, Bollinger Bands, etc.) depending on what you fetch.

1.2. social_data (LunarCrush)

create table public.social_data (
  id                uuid                primary key default uuid_generate_v4(),
  fetched_at        timestamptz         not null default now(),
  coin_slug         text                not null,   -- e.g. 'solana'
  raw_response      jsonb               not null,   -- full JSON from /social and /metrics endpoints
  galaxy_score      numeric(6,3),                   -- parsed
  alt_rank          int,                             -- parsed
  social_volume     numeric(14,2),                   -- parsed
  sentiment         numeric(6,3),                    -- parsed
  social_timestamp  timestamptz,                     -- timestamp included in response (if any)
  -- etc.
  constraint chk_coin_slug check (coin_slug <> '')
);

create index idx_social_data_fetched_at on public.social_data (fetched_at);
create index idx_social_data_slug on public.social_data (coin_slug);

1.3. fundamental_data (CryptoRank real-time)

create table public.fundamental_data (
  id                uuid                primary key default uuid_generate_v4(),
  fetched_at        timestamptz         not null default now(),
  coin_slug         text                not null,   -- e.g. 'solana' or numeric ID
  raw_response      jsonb               not null,   -- full JSON from CryptoRank /currencies/:id
  price_usd         numeric(14,6),                  -- parsed
  market_cap_usd    numeric(18,2),                  -- parsed
  volume_24h_usd    numeric(18,2),                  -- parsed
  circulating_supply numeric(18,2),                 -- parsed
  percent_change_24h numeric(10,4),                 -- parsed
  -- etc.
  constraint chk_coin_slug2 check (coin_slug <> '')
);

create index idx_fundamental_data_fetched_at on public.fundamental_data (fetched_at);
create index idx_fundamental_data_slug on public.fundamental_data (coin_slug);

1.4. historical_data (CryptoRank historical—if you gain access later)

-- If you ever add historical price, time‐series, or sparkline data:
create table public.historical_data (
  id                uuid                primary key default uuid_generate_v4(),
  fetched_at        timestamptz         not null default now(),
  coin_slug         text                not null,   -- e.g. 'solana'
  interval          text                not null,   -- e.g. '5m', '1d'
  raw_response      jsonb               not null,   -- full JSON array of historical points
  -- You can also unnest into a separate normalized structure if needed
  constraint chk_historical check (coin_slug <> '')
);

create index idx_historical_fetched_at on public.historical_data (fetched_at);

(Since your basic plan doesn’t offer historical, this is optional for future)
1.5. onchain_data

create table public.onchain_data (
  id                uuid                primary key default uuid_generate_v4(),
  fetched_at        timestamptz         not null default now(),
  coin_slug         text                not null,       -- normally 'solana'
  raw_response      jsonb               not null,       -- raw JSON from /onchain/metrics & /validators
  tps               numeric(10,3),                      -- parsed
  active_validators int,                                    -- parsed
  staking_yield     numeric(6,3),                       -- parsed
  whale_flow        numeric(14,2),                      -- parsed
  tvl               numeric(18,2),                      -- parsed
  dex_volume        numeric(18,2),                      -- parsed
  -- etc.
  constraint chk_onchain_slug check (coin_slug <> '')
);

create index idx_onchain_data_fetched_at on public.onchain_data (fetched_at);
create index idx_onchain_data_slug on public.onchain_data (coin_slug);

1.6. astrology_data (basic astrology: moon phase, planetary positions, aspects)

create table public.astrology_data (
  id                uuid                primary key default uuid_generate_v4(),
  fetched_at        timestamptz         not null default now(),
  raw_moon_phase    jsonb,           -- raw response from /moon-phase
  raw_planetary     jsonb,           -- raw response from /planetary-positions
  raw_aspects       jsonb            -- raw response from /aspects
);

create index idx_astrology_data_fetched_at on public.astrology_data (fetched_at);

1.7. financial_astrology_data (composite FAI)

create table public.financial_astrology_data (
  id                  uuid              primary key default uuid_generate_v4(),
  fetched_at          timestamptz       not null default now(),
  raw_response        jsonb             not null,
  weighted_aspect     numeric(6,3)       not null,
  ingress_score       numeric(6,3)       not null,
  midpoint_score      numeric(6,3)       not null,
  station_score       numeric(6,3)       not null,
  node_score          numeric(6,3)       not null,
  composite_fai       numeric(6,3)       not null
);

create index idx_fai_fetched_at on public.financial_astrology_data (fetched_at);

1.8. predictions (live_predictions)

You already have live_predictions—just ensure you store the full raw inputs too, or at least reference the IDs of the data that went into this prediction.

-- Extend your existing live_predictions table (or create if missing)
create table if not exists public.predictions (
  id                uuid              primary key default uuid_generate_v4(),
  generated_at      timestamptz       not null default now(),
  -- you might want to store foreign keys referencing the fetched data tables
  technical_id      uuid              references public.technical_data(id),
  social_id         uuid              references public.social_data(id),
  fundamental_id    uuid              references public.fundamental_data(id),
  onchain_id        uuid              references public.onchain_data(id),
  astrology_id      uuid              references public.astrology_data(id),
  fai_id            uuid              references public.financial_astrology_data(id),
  tech_score        numeric(6,3)       not null,
  social_score      numeric(6,3)       not null,
  fundamental_score numeric(6,3)       not null,
  astrology_score   numeric(6,3)       not null,
  composite_score   numeric(6,3)       not null,
  category          text              not null
);

create index idx_predictions_generated_at on public.predictions (generated_at);

1.9. news_scores

create table public.news_scores (
  id                uuid               primary key default uuid_generate_v4(),
  fetched_at        timestamptz        not null default now(),
  raw_response      jsonb              not null,  -- full JSON from /openai/analyze-news
  headline          text,                          -- parsed headline
  sentiment_score   numeric(6,3),                  -- parsed sentiment
  source            text                            -- e.g. “cointelegraph” or “twitter”
);

create index idx_news_scores_fetched_at on public.news_scores (fetched_at);

1.10. daily_updates

create table public.daily_updates (
  id                uuid              primary key default uuid_generate_v4(),
  generated_at      timestamptz       not null default now(),  -- date of summary
  raw_response      jsonb             not null,  -- full JSON from /openai/daily-update
  content           text              not null   -- parsed text summary
);

create index idx_daily_updates_generated_at on public.daily_updates (generated_at);

1.11. dynamic_weights

create table public.dynamic_weights (
  id                uuid              primary key default uuid_generate_v4(),
  generated_at      timestamptz       not null default now(),
  raw_response      jsonb             not null,  -- full JSON from /openai/suggest-weights
  technical_pct     numeric(5,2)       not null,
  social_pct        numeric(5,2)       not null,
  fundamental_pct   numeric(5,2)       not null,
  astrology_pct     numeric(5,2)       not null,
  justification     jsonb             -- full justification object
);

create index idx_dynamic_weights_generated_at on public.dynamic_weights (generated_at);

2. Modifying Your Services to Persist Data

Below are examples of how to augment each existing service so that immediately after fetching API data, you insert a new row into the appropriate Supabase table.

    Note: We assume you already have a configured Supabase client supabase in services/supabaseClient.js (or similar). If not, create one:

    // services/supabaseClient.js
    import { createClient } from "@supabase/supabase-js";

    const supabaseUrl = process.env.SUPABASE_URL;
    const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
    export const supabase = createClient(supabaseUrl, supabaseKey);

2.1. TAAPI Pro (“technical_data”)

In your TAAPI service (e.g. services/taapiService.js), after you fetch responseJson:

// services/taapiService.js
import { supabase } from "./supabaseClient.js";

export async function fetchTaapiBulk(coinSymbol = "SOL/USDT", interval = "1h") {
  // 1) Fetch from TAAPI
  const url = `https://api.taapi.io/bulk`;
  const body = {
    secret: process.env.TAAPI_API_KEY,
    construct: {
      exchange: "binance",
      symbol: coinSymbol,
      interval,
    },
    indicators: [
      { name: "rsi" },
      { name: "macd" },
      { name: "ema", params: { period: 200 } },
      // etc.
    ],
  };

  const res = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
  });
  const data = await res.json();

  // 2) Parse out a few fields (optional)
  const rsiValue = data.indicators.rsi.value;
  const macdHist = data.indicators.macd.histogram;
  const emaValue = data.indicators.ema.value;

  // 3) Insert into Supabase
  const { error } = await supabase.from("technical_data").insert([
    {
      coin_symbol: coinSymbol,
      interval,
      raw_response: data,
      rsi_value: rsiValue,
      macd_histogram: macdHist,
      ema_value: emaValue,
    },
  ]);

  if (error) {
    console.error("Failed to insert technical_data:", error.message);
    // but do not throw—still return original data
  }

  return data;
}

    Call fetchTaapiBulk() wherever you previously fetched TAAPI data.

    This ensures each new fetch automatically writes a row into technical_data.

2.2. LunarCrush (“social_data”)

// services/lunarCrushService.js
import { supabase } from "./supabaseClient.js";

export async function fetchLunarSocial(coinSlug = "solana") {
  const url = `https://lunarcrush.com/api4/public/coins/${coinSlug}/v1`;
  const res = await fetch(url);
  const data = await res.json();

  // Parse out key metrics
  const galaxyScore = data.data.galaxy_score;
  const altRank = data.data.alt_rank;
  const socialVolume = data.data.volatility; // adjust if social volume separate
  const sentiment = data.data.sentiment;

  // Insert into Supabase
  const { error } = await supabase.from("social_data").insert([
    {
      coin_slug: coinSlug,
      raw_response: data,
      galaxy_score: galaxyScore,
      alt_rank: altRank,
      social_volume: socialVolume,
      sentiment,
      social_timestamp: new Date(), // or pick from data if provided
    },
  ]);

  if (error) console.error("Failed to insert social_data:", error.message);

  return data;
}

2.3. CryptoRank (“fundamental_data” + “historical_data”)

// services/cryptoRankService.js
import { supabase } from "./supabaseClient.js";

export async function fetchCryptoRankCurrent(coinSlug = "solana") {
  const url = `https://api.cryptorank.io/v2/currencies/${coinSlug}`;
  const res = await fetch(url, {
    headers: {
      "X-API-KEY": process.env.CRYPTORANK_API_KEY,
      "Content-Type": "application/json",
    },
  });
  const data = await res.json();

  // Parse key fundamentals
  const priceUsd = data.data.price;
  const marketCapUsd = data.data.market_cap;
  const volume24hUsd = data.data.volume_24h;
  const circulatingSupply = data.data.circulating_supply;
  const percentChange24h = data.data.percent_change_24h;

  // Insert into Supabase
  const { error } = await supabase.from("fundamental_data").insert([
    {
      coin_slug: coinSlug,
      raw_response: data,
      price_usd: priceUsd,
      market_cap_usd: marketCapUsd,
      volume_24h_usd: volume24hUsd,
      circulating_supply: circulatingSupply,
      percent_change_24h: percentChange24h,
    },
  ]);

  if (error) console.error("Failed to insert fundamental_data:", error.message);
  return data;
}

// Optional: historical (if you enable later)
export async function fetchCryptoRankHistorical(coinSlug = "solana", interval = "5m") {
  const url = `https://api.cryptorank.io/v2/hist_price/${coinSlug}?interval=${interval}&api_key=${process.env.CRYPTORANK_API_KEY}`;
  const res = await fetch(url);
  if (res.status === 403 || res.status === 404) {
    console.warn("Historical not available for this plan.");
    return null;
  }
  const data = await res.json();

  const { error } = await supabase.from("historical_data").insert([
    {
      coin_slug: coinSlug,
      interval,
      raw_response: data,
    },
  ]);
  if (error) console.error("Failed to insert historical_data:", error.message);
  return data;
}

2.4. On‐Chain Data (“onchain_data”)

// services/onchainService.js
import { supabase } from "./supabaseClient.js";

export async function fetchOnchainMetrics(coinSlug = "solana") {
  // Suppose /api/onchain/metrics returns JSON { tps, active_validators, staking_yield, whale_flow, tvl, dex_volume }
  const res = await fetch(`/api/onchain/metrics?symbol=${coinSlug}`);
  const data = await res.json();

  const { tps, active_validators, staking_yield, whale_flow, tvl, dex_volume } = data;

  const { error } = await supabase.from("onchain_data").insert([
    {
      coin_slug: coinSlug,
      raw_response: data,
      tps,
      active_validators,
      staking_yield,
      whale_flow,
      tvl,
      dex_volume,
    },
  ]);
  if (error) console.error("Failed to insert onchain_data:", error.message);
  return data;
}

2.5. Basic “astrology_data”

// services/astrologyService.js
import { supabase } from "./supabaseClient.js";

export async function fetchAstrologyAll() {
  // 1) Moon Phase
  const moonRes = await fetch("/api/astrology/moon-phase");
  const moonJson = await moonRes.json();

  // 2) Planetary positions
  const planetsRes = await fetch("/api/astrology/planetary-positions");
  const planetsJson = await planetsRes.json();

  // 3) Aspects
  const aspectsRes = await fetch("/api/astrology/aspects");
  const aspectsJson = await aspectsRes.json();

  const { error } = await supabase.from("astrology_data").insert([
    {
      raw_moon_phase: moonJson,
      raw_planetary: planetsJson,
      raw_aspects: aspectsJson,
    },
  ]);
  if (error) console.error("Failed to insert astrology_data:", error.message);

  return { moon: moonJson, planets: planetsJson, aspects: aspectsJson };
}

2.6. Financial Astrology (“financial_astrology_data”)

// services/financialAstrologyService.js
import { supabase } from "./supabaseClient.js";
import { computeFinancialAstrologyIndex } from "./financialAstrology.js";

export async function fetchAndStoreFAI() {
  const now = new Date();
  const result = await computeFinancialAstrologyIndex(now);

  const { weightedAspect, ingressScore, midpointScore, stationScore, nodeScore, compositeFAI } = result;

  const { error } = await supabase.from("financial_astrology_data").insert([
    {
      raw_response: result,
      weighted_aspect: weightedAspect,
      ingress_score: ingressScore,
      midpoint_score: midpointScore,
      station_score: stationScore,
      node_score: nodeScore,
      composite_fai: compositeFAI,
    },
  ]);
  if (error) console.error("Failed to insert financial_astrology_data:", error.message);

  return result;
}

2.7. Predictions (after computing each prediction)

Wherever you compute and store a new prediction (in services/predictionService.js or your scheduler):

// services/predictionService.js
import { supabase } from "./supabaseClient.js";
import { fetchTechnical, fetchSocial, fetchFundamental, fetchOnchainMetrics, fetchAstrologyAll, fetchAndStoreFAI } from "./...";

export async function generateAndStorePrediction() {
  // 1) Fetch each pillar (and store them)
  const taapiData = await fetchTaapiBulk("SOL/USDT", "1h");
  const taapiRow = (await supabase.from("technical_data").select("id").order("fetched_at", { ascending: false }).limit(1)).data[0];
  const socialData = await fetchLunarSocial("solana");
  const socialRow = (await supabase.from("social_data").select("id").order("fetched_at", { ascending: false }).limit(1)).data[0];
  const fundamentalData = await fetchCryptoRankCurrent("solana");
  const fundamentalRow = (await supabase.from("fundamental_data").select("id").order("fetched_at", { ascending: false }).limit(1)).data[0];
  const onchainData = await fetchOnchainMetrics("solana");
  const onchainRow = (await supabase.from("onchain_data").select("id").order("fetched_at", { ascending: false }).limit(1)).data[0];
  const astrologyData = await fetchAstrologyAll();
  const astrologyRow = (await supabase.from("astrology_data").select("id").order("fetched_at", { ascending: false }).limit(1)).data[0];
  const faiData = await fetchAndStoreFAI();
  const faiRow = (await supabase.from("financial_astrology_data").select("id").order("fetched_at", { ascending: false }).limit(1)).data[0];

  // 2) Compute pillar sub-scores (0–100 each)
  const techScore = normalizeTechnical(taapiData);       // e.g. your own function
  const socialScore = normalizeSocial(socialData);
  const fundamentalScore = normalizeFundamental(fundamentalData);
  const astrologyScore = faiData.composite_fai;         // use composite FAI as your astrology score

  // Composite
  const compositeScore = techScore * 0.4 + socialScore * 0.25 + fundamentalScore * 0.2 + astrologyScore * 0.15;
  const category = classify(compositeScore); // e.g. 'Strong Bullish', etc.

  // 3) Insert into predictions
  const { data: predInsert, error: predError } = await supabase.from("predictions").insert([
    {
      technical_id: taapiRow.id,
      social_id: socialRow.id,
      fundamental_id: fundamentalRow.id,
      onchain_id: onchainRow.id,
      astrology_id: astrologyRow.id,
      fai_id: faiRow.id,
      tech_score: techScore,
      social_score: socialScore,
      fundamental_score: fundamentalScore,
      astrology_score: astrologyScore,
      composite_score: compositeScore,
      category,
    },
  ]).select();

  if (predError) {
    console.error("Error storing prediction:", predError.message);
  } else {
    const newPred = predInsert[0];
    console.log(`[Prediction Scheduler] New prediction stored (id=${newPred.id}) at ${newPred.generated_at}`);

    // 4) Immediately invoke dynamic weight suggestion
    const { error: wError } = await supabase.rpc("rpc_suggest_weights", {
      // or directly call your suggest-weights service
      // but if you already extracted logic to a JS service function:
      // await fetchAndStoreWeights(newPred.id)
    });
    if (wError) console.error("Error generating dynamic weights:", wError.message);
  }

  return;
}

    Notes:

        We assume normalizeTechnical(...), normalizeSocial(...), etc., are your existing normalization functions (which you already have).

        We call fetchAndStoreFAI() so that the astrology_score is data‐backed by financial-astrology.

        After inserting a new row in predictions, we either call a stored procedure (rpc_suggest_weights) or directly invoke your fetchAndStoreWeights() function to write into dynamic_weights.

2.8. News Scores (news_scores)

Wherever you handle /api/openai/analyze-news, after getting a GPT response (e.g. a list of { headline, sentiment_score } objects):

// services/openaiNewsService.js
import { supabase } from "./supabaseClient.js";
import OpenAI from "openai";

export async function analyzeNewsAndStore() {
  // 1) Fetch raw news from LunarCrush or your news source
  const newsRes = await fetch("/api/lunarcrush/news?symbol=SOL");
  const newsJson = await newsRes.json(); // array of news items

  // 2) Call OpenAI with your prompt template
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a news sentiment analysis agent..." },
      { role: "user", content: `Analyze the following news items for sentiment: ${JSON.stringify(newsJson)}` },
    ],
  });

  const parsed = JSON.parse(response.choices[0].message.content); 
  // Expect something like: [{ headline: "SOL hits new high", sentiment_score: 0.8, source: "cointelegraph" }, ...]

  // 3) Insert each into news_scores
  for (let item of parsed) {
    const { headline, sentiment_score, source } = item;
    const { error } = await supabase.from("news_scores").insert([
      {
        raw_response: item, // or store entire parsed object
        headline,
        sentiment_score,
        source,
      },
    ]);
    if (error) console.error("Failed to insert news_scores:", error.message);
  }

  return parsed;
}

2.9. Daily Summaries (daily_updates)

// services/openaiDailyUpdateService.js
import { supabase } from "./supabaseClient.js";
import OpenAI from "openai";

export async function generateDailyUpdateAndStore() {
  // 1) Gather latest data points for prompt
  // e.g. fetch /api/analysis/complete or combine raw data from Supabase
  const analysis = await fetch("/api/analysis/complete").then(res => res.json());

  // 2) Call OpenAI to generate summary
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "Generate a concise daily market update for Solana." },
      { role: "user", content: JSON.stringify(analysis) },
    ],
  });

  const content = response.choices[0].message.content;

  // 3) Insert into daily_updates
  const { error } = await supabase.from("daily_updates").insert([
    {
      raw_response: { analysis, content },
      content,
    },
  ]);
  if (error) console.error("Failed to insert daily_updates:", error.message);

  return content;
}

2.10. Dynamic Weights (dynamic_weights)

// services/openaiWeightsService.js
import { supabase } from "./supabaseClient.js";
import OpenAI from "openai";

export async function suggestWeightsAndStore() {
  // 1) Gather latest prediction and data for prompt
  const { data: latestPreds } = await supabase
    .from("predictions")
    .select("*")
    .order("generated_at", { ascending: false })
    .limit(1);

  const latest = latestPreds[0];
  // Optionally also fetch recent news, analysis, etc., to include in prompt

  // 2) Call OpenAI to suggest weights
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "Suggest optimal pillar weights for Solana prediction based on current data." },
      { role: "user", content: JSON.stringify(latest) },
    ],
  });

  const parsed = JSON.parse(response.choices[0].message.content);
  // Expect: { Technical: 40, Social: 30, Fundamental: 20, Astrology: 10, justification: {...} }

  // 3) Insert into dynamic_weights
  const { Technical, Social, Fundamental, Astrology, justification } = parsed;
  const { error } = await supabase.from("dynamic_weights").insert([
    {
      raw_response: parsed,
      technical_pct: Technical,
      social_pct: Social,
      fundamental_pct: Fundamental,
      astrology_pct: Astrology,
      justification,
    },
  ]);
  if (error) console.error("Failed to insert dynamic_weights:", error.message);

  return parsed;
}

3. Scheduling & Ensuring Everything Fires

You already wired up a scheduler in server/index.ts for OpenAI calls. Now, ensure that all of these “fetch & store” functions are invoked inside that scheduler or inside your existing data‐collection loops. A high‐level sketch:

// server/index.ts
import express from "express";
import { fetchTaapiBulk } from "./services/taapiService.js";
import { fetchLunarSocial } from "./services/lunarCrushService.js";
import { fetchCryptoRankCurrent } from "./services/cryptoRankService.js";
import { fetchOnchainMetrics } from "./services/onchainService.js";
import { fetchAstrologyAll } from "./services/astrologyService.js";
import { fetchAndStoreFAI } from "./services/financialAstrologyService.js";
import { generateAndStorePrediction } from "./services/predictionService.js";
import { analyzeNewsAndStore } from "./services/openaiNewsService.js";
import { generateDailyUpdateAndStore } from "./services/openaiDailyUpdateService.js";
import { suggestWeightsAndStore } from "./services/openaiWeightsService.js";

const app = express();

// … your existing middleware/routes …

// 3.1. Poll each data source every X minutes (e.g. 5 or 10 minutes)
setInterval(async () => {
  try {
    console.log(`[Scheduler] Fetching new pillar data at ${new Date().toISOString()}`);
    await fetchTaapiBulk("SOL/USDT", "1h");
    await fetchLunarSocial("solana");
    await fetchCryptoRankCurrent("solana");
    await fetchOnchainMetrics("solana");
    await fetchAstrologyAll();
    await fetchAndStoreFAI();
    // After all pillars are fetched and stored, compute & store new prediction
    await generateAndStorePrediction();
  } catch (err) {
    console.error("[Scheduler] Error in data‐collection loop:", err);
  }
}, 10 * 60 * 1000); // every 10 minutes

// 3.2. Hourly news sentiment
setInterval(async () => {
  try {
    console.log(`[Scheduler] Running analyze-news at ${new Date().toISOString()}`);
    await analyzeNewsAndStore();
  } catch (err) {
    console.error("[OpenAI Scheduler] News analysis failed:", err);
  }
}, 60 * 60 * 1000); // every hour

// 3.3. Daily summary at midnight UTC
const scheduleDailySummary = () => {
  const now = new Date();
  const nextMidnightUtc = new Date(Date.UTC(now.getUTCFullYear(), now.getUTCMonth(), now.getUTCDate() + 1, 0, 0, 0));
  const msUntilMidnight = nextMidnightUtc.getTime() - now.getTime();
  console.log(`[OpenAI Scheduler] Daily summary scheduled in ${Math.floor(msUntilMidnight / 60000)} minutes`);
  setTimeout(async () => {
    try {
      console.log(`[OpenAI Scheduler] Running daily-update at ${new Date().toISOString()}`);
      await generateDailyUpdateAndStore();
    } catch (err) {
      console.error("[OpenAI Scheduler] Daily update failed:", err);
    }
    // schedule next
    scheduleDailySummary();
  }, msUntilMidnight);
};
scheduleDailySummary();

app.listen(process.env.PORT || 5000, () => {
  console.log("Server listening on port 5000");
});

    Every 10 minutes you fetch raw pillar data and store it plus run a new prediction.

    Every hour you analyze news via OpenAI and store the results.

    Every midnight UTC you generate a daily summary.

You can adjust the “10 minutes” polling to whatever cadence you prefer (e.g. 5 minutes), as long as you don’t exceed API rate limits.
4. Indexing & Best Practices for Future ML

    Index on fetched_at / generated_at

        Each table has an index on the timestamp so that you can quickly retrieve time‐series slices (e.g. “all technical_data for June 2025”).

    Partition large tables if needed

        If you plan to collect data for months or years, tables like technical_data and social_data can grow very large. Supabase (PostgreSQL) supports partitioning by date to keep queries fast. For example:

        CREATE TABLE technical_data_y2025 PARTITION OF technical_data FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');
        CREATE TABLE technical_data_y2026 PARTITION OF technical_data FOR VALUES FROM ('2026-01-01') TO ('2027-01-01');

        Or simply prune older data if you only need a rolling window.

    Normalize when necessary, but store raw JSON

        Keeping the full raw_response allows you to re‐derive any features later. At the same time, parsed columns (rsi_value, galaxy_score, etc.) let you quickly train models without unnesting JSON each time.

    Timestamp consistency

        Use timestamptz in UTC. Always call now() or supply new Date().toISOString(). That way, your ML pipelines can merge on a consistent time axis.

    Foreign keys for reproducibility

        By linking predictions.technical_id → technical_data.id, you know exactly which row (and timestamp) contributed to a given prediction. This makes backtests fully traceable.

    Data retention policy

        Early on, keep all data. Once you have enough (e.g. 6–12 months), consider archiving very high‐frequency tables (like onchain or technical if polled every minute) into a cold store, leaving only daily aggregates available for “live” queries.

5. Querying for ML Pipelines

When you’re ready to train a model (say, to predict Solana’s next‐hour return or classify “true bull runs”), you can:

    Pull features:

select 
  t.fetched_at as ts, 
  t.rsi_value,
  t.macd_histogram,
  f.price_usd,
  f.volume_24h_usd,
  s.galaxy_score,
  s.sentiment,
  o.tps,
  o.tvl,
  a.composite_fai,
  p.composite_score as label  -- your target might be next‐hour composite or future price
from technical_data t
join fundamental_data f on f.fetched_at = t.fetched_at  -- use a time‐bucket join, or join on nearest ts
join social_data s on s.fetched_at = t.fetched_at
join onchain_data o on o.fetched_at = t.fetched_at
join financial_astrology_data a on a.fetched_at = t.fetched_at
join predictions p on p.generated_at = t.fetched_at
where t.fetched_at between '2025-01-01' and '2025-06-01';

    You may need to join on “nearest timestamp” rather than exact match; e.g. leverage a window function or do:

    select 
      t.fetched_at,
      t.rsi_value,
      s.galaxy_score,
      p.composite_score
    from technical_data t
    left join lateral (
      select galaxy_score 
      from social_data s2 
      where abs(extract(epoch from s2.fetched_at - t.fetched_at)) < 300  -- within 5 minutes
      order by abs(extract(epoch from s2.fetched_at - t.fetched_at)) asc 
      limit 1
    ) s on true
    left join lateral (
      select composite_score 
      from predictions p2 
      where p2.generated_at > t.fetched_at
      order by p2.generated_at asc
      limit 1
    ) p on true

    That lets you align the “target” (next prediction) with the current features.

Exporting

    In Supabase you can expose a Postgres federated view or just download a CSV from the SQL editor:

        copy (
          select * from (
            -- complicated join query for ML features
          ) x
        ) to '/tmp/solana_ml_dataset.csv' csv header;

        Then download from Replit or use supabase.storage to store the CSV.

    Automated ETL Pipelines

        Eventually, you may build a separate Node script (or Python) that runs daily, extracts the last 30 days of features + labels, trains a model (e.g. scikit-learn, TensorFlow), and re‐uploads new model weights. But that’s a separate effort once you have the data.

6. Summary

    Extended your Supabase schema to capture every pillar’s raw and parsed data in its own table.

    Modified each service so that immediately after fetching an API response, you write a new row into Supabase (technical_data, social_data, etc.).

    Linked predictions to the exact rows (via foreign keys) so that every prediction’s inputs are fully traceable.

    Added logging and indices so that time‐series queries will be fast.

    Outlined how to query these tables later for ML training, including joining on nearest timestamps.

With this in place, you will have a robust historical record of every data point—perfect for training future machine‐learning models, backtesting strategies, or simply charting long‐term trends.